{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "# num_qubits = 4\n",
        "# X = StandardScaler().fit_transform(data)\n",
        "# pca = PCA(num_qubits)\n",
        "# X = pca.fit_transform(X)\n",
        "\n",
        "# scaler = MinMaxScaler((-3.142, 3.142))\n",
        "# X_scaled = scaler.fit_transform(X)\n",
        "# X_train_0, X_test_0, y_train_0, y_test_0 = train_test_split(\n",
        "# X_scaled, targets, test_size=0.25, random_state=2)"
      ],
      "metadata": {
        "id": "-xS3gRYwaHsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "class MyNeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyNeuralNetwork, self).__init__()\n",
        "        self.tn_layer = TNLayer()\n",
        "        self.vqc = VQC(4, 2)\n",
        "        #self.vqc = VQC(feature_map=FeatureMap(), ansatz=VarCirc(), loss=\"cross_entropy\", optimizer= COBYLA(maxiter=50), quantum_instance=quantum_instance,)\n",
        "        #self.output_layer = nn.Linear(2, 2)  # Adjust the output layer to match the number of classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tn_layer(x)\n",
        "        #print(x.shape)\n",
        "        x = x.reshape(len(x), 784)\n",
        "        #print(x.shape)\n",
        "        #print(\"++++++\")\n",
        "        #print(x)\n",
        "        scaler = MinMaxScaler((-3.142, 3.142))\n",
        "        y = StandardScaler().fit_transform(x)\n",
        "        pca = PCA(4)\n",
        "        X1 = pca.fit_transform(y)\n",
        "        X_scaled = scaler.fit_transform(X1)\n",
        "        X_train_0, X_test_0, y_train_0, y_test_0 = train_test_split(X_scaled, targets, test_size=0.25, random_state=2)\n",
        "        #print(X1.shape)\n",
        "        print(\"++++++\")\n",
        "        #print(X1[0])\n",
        "        Y1 =[]\n",
        "        for i in range(100):\n",
        "        #X1 = self.vqc(VQC.create_zero_tensor(4, 2), X1.view(X1.shape[0], -1))\n",
        "           Y1.append(self.vqc(VQC.create_zero_tensor(4, 2), torch.tensor(X1[i])))\n",
        "        #X1 = self.vqc(VQC.create_zero_tensor(4, 2), X1)\n",
        "        # feature_map = FeatureMap()\n",
        "        # variational_circuit = VarCirc()\n",
        "        #X1 = VQC(feature_map=FeatureMap(), ansatz=VarCirc(), loss=\"cross_entropy\", optimizer= COBYLA(maxiter=50), quantum_instance=quantum_instance,)\n",
        "    #optimizer = ADAM(maxiter=30, lr=1e-3, tol = 1e-6),\n",
        "        #X1 = self.output_layer(X1)\n",
        "        #print(Y1)\n",
        "        #print(Y1.shape)\n",
        "        Y1=torch.stack(Y1)\n",
        "        #print(Y1.shape)\n",
        "        return Y1"
      ],
      "metadata": {
        "id": "MvP5aDurab74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the neural network\n",
        "net = MyNeuralNetwork()\n",
        "# Define a loss function and an optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.01, weight_decay=0.001)\n",
        "\n",
        "# Training the neural network\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "  with torch.no_grad():\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(data)\n",
        "    # print(outputs)\n",
        "    # print(outputs.shape)\n",
        "    #print(labels.shape)\n",
        "    Z=torch.squeeze(outputs, 1)\n",
        "    #Z = Z.type(torch.int32)\n",
        "    # print(Z)\n",
        "    # print(Z.shape)\n",
        "    #T = torch.targets(100, 1,3)\n",
        "    #torch.reshape(torch.targets, (100, 1, 3))\n",
        "    targets=torch.tensor(targets)\n",
        "    # print(targets)\n",
        "    # print(targets.shape)\n",
        "    loss = criterion(Z, targets)\n",
        "    loss = Variable(loss, requires_grad = True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
        "    #loss_list_train.append(loss.clone())\n",
        "    #accuracy_list_train.append(accuracy/100)"
      ],
      "metadata": {
        "id": "yarxbDKya2I9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}